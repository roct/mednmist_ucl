{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from medmnist import PathMNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://zenodo.org/records/6496656/files/pathmnist.npz to /Users/ramx/.medmnist/pathmnist.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205615438/205615438 [00:42<00:00, 4833520.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/ramx/.medmnist/pathmnist.npz\n",
      "Using downloaded and verified file: /Users/ramx/.medmnist/pathmnist.npz\n"
     ]
    }
   ],
   "source": [
    "dataset_train = PathMNIST(split='train', download=True)\n",
    "dataset_test = PathMNIST(split='test', download=True)\n",
    "dataset_val = PathMNIST(split='val', download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset lengths, train:89996 - test:7180 - val:10004\n",
      "Shape and type of each object: 2, <class 'tuple'>\n",
      "Shape and type of image: (28, 28, 3), <class 'PIL.Image.Image'>\n",
      "Shape and type of label: (1,), <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset lengths, train:{} - test:{} - val:{}\".format(len(dataset_train), len(dataset_test), len(dataset_val)))\n",
    "print(\"Shape and type of each object: {}, {}\".format(len(dataset_train[0]), type(dataset_train[0])))\n",
    "print(\"Shape and type of image: {}, {}\".format(np.array(dataset_train[0][0]).shape, type(dataset_train[0][0])))\n",
    "print(\"Shape and type of label: {}, {}\".format(dataset_train[0][1].shape, type(dataset_train[0][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAPOklEQVR4nO3c0Y4dV1YG4FV1utvuxE6YkIlGowwjgeCW938MHgAuENIERAYncRy7+5wqLjxaSDBSn/WjLhL0fddZZ1ft2uW/6yL/su/7XgBQVev/9QUA8PMhFABoQgGAJhQAaEIBgCYUAGhCAYAmFABoN9f+h//4D/8y/vHkf4tb5iMf59b5ZJKIW3BTW7APaVrvNV9sD3Z9WY77fx73LRha5ve0Bw8qOHaV3E5V9m4cdX3LQftdVXU6BWtFuxe8S+Frkcwlz/av//7rp393/rMA/H8lFABoQgGAJhQAaEIBgCYUAGhCAYAmFABoQgGAJhQAaEIBgCYUAGhXF+JtB5WFpYVSSYvX5ZiOrKzkL20GPOj6orLDoDTt42LzhxstFZT8bcHureE+ZM/pmOLCbL+ztc7vH8czb7/9fjzz4TxfJynMrKp6fAzWOs/fC4V4AIwIBQCaUACgCQUAmlAAoAkFAJpQAKAJBQCaUACgCQUAmlAAoAkFANrVhXhrEh9JN9RBBV5VFbV4JcVfS9L8FZSzfVxr/qCOekxxx1/QrJh1HR7TkBh0N1ZV1Sk5r8E9JbuQlNR994c/BitVrZdgB19c/U9du335Yjxzc3saz1RV7cGmr2u21pO/+yy/CsAvklAAoAkFAJpQAKAJBQCaUACgCQUAmlAAoAkFAJpQAKAJBQCaUACgCQUA2vXVgUEr5rbN2wyjRtHKmj6TItLlsBjN9mE7sr70IKegJTW5qWWb792S1OZGpzWbe/zwMJ754zffjmcuP81bUj//6ovxTFXV/V98Op5ZgjMUvUpHvkvPtJgvBQCaUACgCQUAmlAAoAkFAJpQAKAJBQCaUACgCQUAmlAAoAkFAJpQAKBdX4gXCQqbwpKnNSi8Ssrjtst4pNYgetPKtOSeshLC+Tp7WvIXFNUle56cocTlPC+KrKp6869vxjPv3r4dz7z+8vPxzKu/ej2eWZOHVNl5yFofgzMevrjJ2dvPwT9GV/ClAEATCgA0oQBAEwoANKEAQBMKADShAEATCgA0oQBAEwoANKEAQBMKALSrC/GWoKju5ua4zImqzIJ72qMyrrm0mi3pdEvuaA32Lt265Oxl68xnkgK07d3DfKiq7u9uxzO/+tvfjWeWpJztmNfio+Q4BDOnoLDv4UP2bL//jx/GM3ePyab//sn/wpcCAE0oANCEAgBNKADQhAIATSgA0IQCAE0oANCEAgBNKADQhAIATSgA0K4uxMsKr+ZDafnZHtW6ZStNXbb5KknhXNVx5XGJpKyvqqImvS2Y2U/zC0z+qvrx378LpqpON1e/ru27f/52PLOdL+OZ+9f345k9bdELztF2ma/1+DAvt/v+hx/HM1VVL16/HM+8vP8kWuspvhQAaEIBgCYUAGhCAYAmFABoQgGAJhQAaEIBgCYUAGhCAYAmFABoQgGAdn0hXlQ4N2+u2oMis3CpWoJ72oNirWTv9jXL61MwtyUlf+/P83U+zGeqquoxmAvO0fufPoxnfvzh7Xjmk199Np6pqrr/4tV4Zj2dxjPv37wbz9y+uB3PrC/mBX9VVQ8fHscz+zZvpbz/bF7y99nvfj2eqaq6uZm/t5f38324hi8FAJpQAKAJBQCaUACgCQUAmlAAoAkFAJpQAKAJBQCaUACgCQUAmlAAoAkFANrVNYVL1EI6l7WxVgUliHU6za9wvUnu6jjnnx7GM2//7c145nQ7b8W8C5o0q6q2h3lL6vndvPH0dD+/vq/+7uvxzHqX7UPS6pu8Fy+/+HQ8s56Cvy/DV+nly3m76p40Nu9hY3PgErT6rrdZy+yTv/ssvwrAL5JQAKAJBQCaUACgCQUAmlAAoAkFAJpQAKAJBQCaUACgCQUAmlAAoF3dqLQu8/zICqWylqw1iLckES+Pl/HM+zfvxjM/vXk7nqmqOm/z63v968/HM3ev7scz9W5e1ldVtV3mrW73v5nf0/rpi/HMkUVrydQWrHVzk7zr831ICv4+rjWfWYN/VvakBTSUPKctKNG7hi8FAJpQAKAJBQCaUACgCQUAmlAAoAkFAJpQAKAJBQCaUACgCQUAmlAAoF1diHd592H840vUUpeVUJ3fP45nkhKqx5/m6yS9Wp9//cV8qKrWl3fjmSW4wD0o41o+mRfOVVWdPgvK94JzFNWLBWcoKdGryqoioyK4YCOSdyl81aPruwRPdwnWCXoBc+kGPvWzz/KrAPwiCQUAmlAAoAkFAJpQAKAJBQCaUACgCQUAmlAAoAkFAJpQAKAJBQDa1YV4+yVoh3r/MB758c2P83Wq6vRqXrZ292petHZ3Py+cW2+OKwZMWt2Wo8rj7k7JVNQoGBX2jSeyMsHg0v60WDAS7V2wznwk3odTcF6Twr4jy+324PqSZ3sNXwoANKEAQBMKADShAEATCgA0oQBAEwoANKEAQBMKADShAEATCgA0oQBAEwoAtKtbUtdP5+2g+/kynnn1+uV4pqpqSZpIo4XmI0nr5Bq0JlZVnbf5Yqdl3l6aFDQuYSvmJbinqC02Kp1MhrKNSI7EsiTvRdAoemDLZzKVlA5HDbhh9WtanPscfCkA0IQCAE0oANCEAgBNKADQhAIATSgA0IQCAE0oANCEAgBNKADQhAIA7epCvKi86mZetFZpodRlXpq2BiV6yT5can5t4TbUEtSF7dFiwTpp7VdUDhiUmQWrrMF5CLsOs1K3ZKGwqG4q2buq7BxFRXVH7ffPjC8FAJpQAKAJBQCaUACgCQUAmlAAoAkFAJpQAKAJBQCaUACgCQUAmlAAoF1diJeUZK1JKdma1JJlRXXbZV6StQRlXMm1hZ1ptQataXuwD1vy58SRJX/BOsl53YN2u7QI7qCeukh6XjMHNdUFzzYqDq2qUzL3TAfClwIATSgA0IQCAE0oANCEAgBNKADQhAIATSgA0IQCAE0oANCEAgBNKADQri7Ei6qXgqF1OS6nLufzeGY9HXN9e9x1NR9cTknZYSK7qT0pIQzWiY5e1ASX1cclU1GZYFAEl8yk5yGRlBAm7+C+Zc92XYPD90zb50sBgCYUAGhCAYAmFABoQgGAJhQAaEIBgCYUAGhCAYAmFABoQgGAJhQAaFcX4mUOLMlKyquCy3t8uIxnTjfz7F3WbB+icrugLGwLNm/btvFMVVUF1xeM1H4Jr28qubjKyu225B0Mnu1ynr8XdVC5ZFVY2Bc8poNO0EdZ996TfCkA0IQCAE0oANCEAgBNKADQhAIATSgA0IQCAE0oANCEAgBNKADQhAIATSgA0J61JXUJ2iCjNsOq2oN6wpsX89vfHs7jmfXmNJ7Zw0bRZPv2ZT4UlU4u6d8gyU3NR5LrSwpP0ybNZyrF/DPrBOchainO7ijpmE1afZObCm+p9i1opg0LpZ/iSwGAJhQAaEIBgCYUAGhCAYAmFABoQgGAJhQAaEIBgCYUAGhCAYAmFABoz1qIl5RDBb1Qf1prPngKGqWWoNwuqTJb1qzt6qiKsSUpTTvs6qr25NmGez5eJzzjW9L6GOzenuz4KXkvjjvjz1Ue9z/WOXDwuc6rLwUAmlAAoAkFAJpQAKAJBQCaUACgCQUAmlAAoAkFAJpQAKAJBQCaUACgPW8hXlIEl66VlO8l1VpBs1ayD0nBX1WF7XGHjGQPqaqW0/xvl+WgBrTwKWVrZZs+nkj2LulmiwsSk+tLigGTfx7iP7OD60vbQ5/gSwGAJhQAaEIBgCYUAGhCAYAmFABoQgGAJhQAaEIBgCYUAGhCAYAmFABoz1qItxzVzhY6rmBsvkq2d9k9nS/beGYNGtCSYruqqiVYK9mHpIQw2Yf5bn90VMlf9mIcU6KXSu4oOQ/5v14HlmY+wZcCAE0oANCEAgBNKADQhAIATSgA0IQCAE0oANCEAgBNKADQhAIATSgA0J61EC9xXB1eUuEVDh14U1FnWlLYd2CZWSK5vj3YiOcqJftzkpLEfU/r92Yu23yd9AwlxYBRmWDwbLfwOEQFk8/0EvpSAKAJBQCaUACgCQUAmlAAoAkFAJpQAKAJBQCaUACgCQUAmlAAoAkFAJpQAKBd3ZJ6OZ/HP34JKgO382U8U1V1e3c7H0paBoOZpKkyacT8uNZ8Zj3N/zY4siw2uac9WC1ZZwuG1iX7W2wNxrYtOK/Rk0paPoNlKns3DnrV45tak39XopWe5ksBgCYUAGhCAYAmFABoQgGAJhQAaEIBgCYUAGhCAYAmFABoQgGAJhQAaFcX4n3zT9+Mf/x8mZfo3X96P56pqvrL3345nllPUePVeGKLSvTGI1VVtQSDS1I4F9zTkjagBfZgH7Z5b2HUf5YWmSW7t67HnL0tKb+cL1NVVTfBe5uU/C3B3i3hTUXX90zvky8FAJpQAKAJBQCaUACgCQUAmlAAoAkFAJpQAKAJBQCaUACgCQUAmlAAoF1diPebv/nt+MePqz+rrJkskBStJcVV2+O8TLCq6uEPb8Yz54f5Wp/8fl5AeHpxO56pCvc8OH1r8CdScm1pJV6yVFaaFswErYpr+s4mY9t86LLP2+32oBjw4+B85Ob2lK31BF8KADShAEATCgA0oQBAEwoANKEAQBMKADShAEATCgA0oQBAEwoANKEAQLu6EC8p/trOl/HMTViadgmKqIK+q6hobQnars7fvh3PVFXdJPf08m4+c5r/PZGVx1VlHWNRe1ywzqG1j3MHFThG5XYH9uEtwZ+/yx6c8aAYsCqrSNzDYsWn+FIAoAkFAJpQAKAJBQCaUACgCQUAmlAAoAkFAJpQAKAJBQCaUACgCQUAmlAAoF3dkvr4w4f5j7+8+uf/1w5raQwqO99/+8N45ub943imqupyN9/zuy9fzxcKChr3rO601jV5tvOZpMQ1uba0LTZpL92TKuDg8uZ9yFWnYKYqe07Jnif7nYqWep6SVF8KAPwXoQBAEwoANKEAQBMKADShAEATCgA0oQBAEwoANKEAQBMKADShAEC7uj1tPc3z4/TiuEK8JSgmS3rWtqDU7UNQiHf36n48U1W1fjUvtzu9uB3PXM7zCrS0CC5pLlz2gxrGgntKt2F/rga0/yaqgUv2IVmnwnMUlQkes99VVUt0S89T2OdLAYAmFABoQgGAJhQAaEIBgCYUAGhCAYAmFABoQgGAJhQAaEIBgCYUAGjLfmTrEwA/a74UAGhCAYAmFABoQgGAJhQAaEIBgCYUAGhCAYAmFABo/wkNeBhvt3uAXwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the first image from the training set\n",
    "first_image = dataset_train[0][0]\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(first_image)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preping the dta for training\n",
    "- Merge training and validation into \"train_val_images\" and \"train_val_labels\"\n",
    "- Extract data and labels into separate numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_val = dataset_train + dataset_val\n",
    "train_val_images = np.array([np.array(dataset_train_val[n][0]) for n in range(len(dataset_train_val))])\n",
    "train_val_labels = np.array([np.array(dataset_train_val[n][1]) for n in range(len(dataset_train_val))])\n",
    "test_images = np.array([np.array(dataset_test[n][0]) for n in range(len(dataset_test))])\n",
    "test_labels = np.array([np.array(dataset_test[n][1]) for n in range(len(dataset_test))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It can be seen that the labels are now integers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [4],\n",
       "       [7],\n",
       "       [5],\n",
       "       [5]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_val_labels.min())\n",
    "display(train_val_labels.max())\n",
    "display(train_val_labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Images have the 3 RGB channels, we could convert to Black and White but the hue may be important for the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 28, 28, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_val_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First exploratory model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN = tf.keras.Sequential([\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 3), padding='same', strides=1),\n",
    "    # layers.MaxPool2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(9, activation='softmax') # Softmax is used for multiclass classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(), # SparseCategoricalCrossentropy is used for multiclass classification with integer labels\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN model\n",
    "training_CNN = model_CNN.fit(x=train_val_images, y=train_val_labels, epochs=5, batch_size=32, validation_split=0.2, verbose=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Convolutional NN evaluation score: 0.6000000238418579'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_eval_CNN = model_CNN.evaluate(test_images, test_labels, verbose=0)\n",
    "display(f\"Convolutional NN evaluation score: {y_eval_CNN[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_CNN = model_CNN.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.92410516e-06, 2.35304502e-11, 5.50155900e-02, 7.56942836e-06,\n",
       "        2.65348759e-02, 3.09030235e-01, 1.99619025e-01, 1.10979518e-02,\n",
       "        3.98692816e-01],\n",
       "       [2.95152701e-02, 6.11575233e-05, 1.67010315e-02, 5.99245774e-04,\n",
       "        7.14727819e-01, 1.18322410e-01, 5.17212525e-02, 5.25400154e-02,\n",
       "        1.58117991e-02],\n",
       "       [2.58447289e-01, 4.21708822e-01, 1.22097717e-03, 8.75253318e-06,\n",
       "        2.95462757e-01, 1.46831460e-02, 2.60664965e-03, 1.69010484e-03,\n",
       "        4.17152233e-03],\n",
       "       [7.42340781e-05, 5.70244574e-10, 1.19907930e-01, 2.62010038e-01,\n",
       "        1.03948582e-02, 9.13021937e-02, 2.51832187e-01, 1.30642116e-01,\n",
       "        1.33836433e-01],\n",
       "       [5.80256805e-02, 2.44016526e-04, 1.45486388e-02, 9.57982493e-08,\n",
       "        1.14102498e-01, 4.23374772e-02, 4.14320640e-02, 3.74152551e-05,\n",
       "        7.29272127e-01],\n",
       "       [2.86684871e-01, 6.04543742e-03, 1.40506536e-05, 1.11948623e-07,\n",
       "        7.07249165e-01, 8.86493157e-10, 6.18769036e-07, 3.64466882e-06,\n",
       "        2.08130928e-06],\n",
       "       [1.87197307e-04, 1.62727971e-04, 1.16059944e-01, 1.27747899e-03,\n",
       "        2.82877628e-02, 1.43172935e-01, 1.02289103e-01, 1.63785160e-01,\n",
       "        4.44777668e-01],\n",
       "       [9.58876491e-01, 3.24793272e-02, 1.04938321e-04, 3.18402294e-14,\n",
       "        8.55672115e-04, 7.28715444e-03, 7.87545650e-06, 1.32067601e-08,\n",
       "        3.88475339e-04],\n",
       "       [1.50343042e-03, 2.11608162e-11, 8.57245699e-02, 2.00096503e-01,\n",
       "        6.71458766e-02, 4.20376323e-02, 4.09383953e-01, 7.01656565e-02,\n",
       "        1.23942450e-01],\n",
       "       [1.29132792e-02, 8.74315845e-07, 8.76525864e-02, 3.42613191e-08,\n",
       "        7.37021258e-03, 5.37124872e-01, 7.77825527e-03, 1.62559154e-05,\n",
       "        3.47143650e-01]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[8],\n",
       "       [4],\n",
       "       [4],\n",
       "       [8],\n",
       "       [4],\n",
       "       [4],\n",
       "       [8],\n",
       "       [0],\n",
       "       [6],\n",
       "       [8]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y_pred_CNN[:10])\n",
    "display(test_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tunning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now using Random Search instead of Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_CNN_model(num_layers=1, nodes=64, activation='relu', learning_rate=0.001):\n",
    "    model = tf.keras.Sequential()\n",
    "    for i in range(num_layers):\n",
    "        if i == 0:\n",
    "            model.add(layers.Conv2D(nodes, activation=activation, kernel_size=(3, 3), input_shape=(28, 28, 3)))\n",
    "        else:\n",
    "            model.add(layers.Conv2D(nodes, activation=activation, kernel_size=(3, 3)))\n",
    "    # model.add(layers.MaxPool2D()),\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(9, activation='softmax'))\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def tune_model(X, y, param_distributions):\n",
    "    model = KerasClassifier(build_fn=create_CNN_model, verbose=0)\n",
    "    rand_search = RandomizedSearchCV(estimator=model, param_distributions=param_distributions, cv=5)\n",
    "    rand_search.fit(X, y)\n",
    "    return rand_search.best_params_, rand_search.best_score_\n",
    "\n",
    "param_distributions = {'num_layers': [3, 4, 5], 'nodes':[32], 'activation': ['relu', 'tanh'], 'batch_size': [32], \n",
    "              'epochs': [5, 8, 11], 'learning_rate': [0.001]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# def tune_model(X, y, param_grid):\n",
    "#     model = KerasClassifier(build_fn=create_CNN_model, verbose=0)\n",
    "#     rand_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, cv=5)\n",
    "#     rand_search.fit(X, y)\n",
    "#     return rand_search.best_params_, rand_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x5/lk3vcxsd1ybc_z4fyxmp024h0000gn/T/ipykernel_55574/4179812120.py:18: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=create_CNN_model, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'num_layers': 5, 'nodes': 32, 'learning_rate': 0.001, 'epochs': 11, 'batch_size': 32, 'activation': 'relu'}\n",
      "Best Score: 0.6627199947834015\n"
     ]
    }
   ],
   "source": [
    "best_params, best_score = tune_model(train_val_images, train_val_labels, param_distributions)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an optimal model according with the previous hyperparameter tunning:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 2s 7ms/step - loss: 0.7044 - accuracy: 0.8074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Optimal CNN evaluation score: 0.8073816299438477'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimal_model = create_CNN_model(num_layers=5, nodes=32, activation='relu', learning_rate=0.001)\n",
    "optimal_model.fit(train_val_images, train_val_labels, epochs=11, batch_size=32, validation_split=0.2, verbose=0)\n",
    "optimal_eval = optimal_model.evaluate(test_images, test_labels, verbose=1)\n",
    "display(f\"Optimal CNN evaluation score: {optimal_eval[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run early stopping for further tunning of Epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1250/1250 [==============================] - 82s 65ms/step - loss: 1.4774 - accuracy: 0.4490 - val_loss: 1.2483 - val_accuracy: 0.5051\n",
      "Epoch 2/15\n",
      "1250/1250 [==============================] - 81s 65ms/step - loss: 1.2128 - accuracy: 0.5445 - val_loss: 1.3836 - val_accuracy: 0.5093\n",
      "Epoch 3/15\n",
      "1250/1250 [==============================] - 72s 58ms/step - loss: 1.1088 - accuracy: 0.5859 - val_loss: 1.1310 - val_accuracy: 0.5828\n",
      "Epoch 4/15\n",
      "1250/1250 [==============================] - 70s 56ms/step - loss: 1.0734 - accuracy: 0.6025 - val_loss: 1.4081 - val_accuracy: 0.5017\n",
      "Epoch 5/15\n",
      "1250/1250 [==============================] - 72s 58ms/step - loss: 1.0399 - accuracy: 0.6162 - val_loss: 1.2669 - val_accuracy: 0.5692\n",
      "Epoch 6/15\n",
      "1250/1250 [==============================] - 74s 59ms/step - loss: 1.0078 - accuracy: 0.6319 - val_loss: 1.2643 - val_accuracy: 0.5857\n",
      "Epoch 7/15\n",
      "1250/1250 [==============================] - 73s 58ms/step - loss: 0.9776 - accuracy: 0.6457 - val_loss: 1.1977 - val_accuracy: 0.5853\n",
      "Epoch 8/15\n",
      "1250/1250 [==============================] - 73s 59ms/step - loss: 0.9321 - accuracy: 0.6585 - val_loss: 1.0850 - val_accuracy: 0.6227\n",
      "Epoch 9/15\n",
      "1250/1250 [==============================] - 73s 58ms/step - loss: 0.9037 - accuracy: 0.6719 - val_loss: 1.1474 - val_accuracy: 0.5921\n",
      "Epoch 10/15\n",
      "1250/1250 [==============================] - 72s 58ms/step - loss: 0.9026 - accuracy: 0.6751 - val_loss: 1.1395 - val_accuracy: 0.6064\n",
      "Epoch 11/15\n",
      "1250/1250 [==============================] - 74s 59ms/step - loss: 0.8743 - accuracy: 0.6842 - val_loss: 1.1153 - val_accuracy: 0.6216\n",
      "Epoch 12/15\n",
      "1250/1250 [==============================] - 73s 58ms/step - loss: 0.8644 - accuracy: 0.6910 - val_loss: 1.1975 - val_accuracy: 0.6136\n",
      "Epoch 13/15\n",
      "1250/1250 [==============================] - 72s 58ms/step - loss: 0.8420 - accuracy: 0.6982 - val_loss: 1.2812 - val_accuracy: 0.5845\n",
      "225/225 [==============================] - 1s 5ms/step - loss: 1.1223 - accuracy: 0.6492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Optimal CNN evaluation score: 0.6491643190383911'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "monitor_val_acc = EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "\n",
    "early_stopping_model = create_CNN_model(num_layers=3, nodes=32, activation='tanh', learning_rate=0.001)\n",
    "early_stopping_model.fit(train_val_images, train_val_labels, epochs=15, batch_size=64, callbacks=[monitor_val_acc], validation_split=0.2, verbose=1)\n",
    "optimal_eval = early_stopping_model.evaluate(test_images, test_labels, verbose=1)\n",
    "display(f\"Optimal CNN evaluation score: {optimal_eval[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labucl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
